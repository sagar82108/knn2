{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "734eb011-4a21-43a4-9e3c-4efdb060d583",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. The main difference between Euclidean and Manhattan distance metrics in KNN is how they measure distance between data points:\n",
    "   - Euclidean distance measures the straight-line distance between points.\n",
    "   - Manhattan distance measures distance by summing the absolute differences along each dimension. \n",
    "   The choice of distance metric can affect KNN performance, with Euclidean being sensitive to differences in scale and Manhattan being less affected by outliers.\n",
    "\n",
    "Q2. To choose the optimal value of k for a KNN classifier or regressor, you can use techniques like cross-validation, grid search, or the elbow method. Cross-validation involves testing different k values on a validation set to find the one that yields the best performance.\n",
    "\n",
    "Q3. The choice of distance metric can impact KNN performance. Euclidean distance may work well when data distribution is roughly spherical, while Manhattan distance is suitable when data follows a grid-like structure or when outliers should have less influence. The choice depends on the problem and data characteristics.\n",
    "\n",
    "Q4. Common hyperparameters in KNN classifiers and regressors include:\n",
    "   - K (number of neighbors)\n",
    "   - Distance metric (e.g., Euclidean or Manhattan)\n",
    "   - Weighting scheme (e.g., uniform or distance-weighted)\n",
    "   Tuning these hyperparameters can be done through techniques like grid search or randomized search, testing different values to find the best combination for model performance.\n",
    "\n",
    "Q5. The size of the training set can impact KNN performance. Too few data points can lead to overfitting, while too many can increase computational complexity. Techniques like cross-validation and learning curves can help optimize the training set size by balancing model performance and computational cost.\n",
    "\n",
    "Q6. Drawbacks of KNN:\n",
    "   - Sensitivity to the choice of K.\n",
    "   - Computationally expensive for large datasets.\n",
    "   - Poor performance in high-dimensional spaces.\n",
    "   - Sensitivity to irrelevant features.\n",
    "   To overcome these drawbacks, you can:\n",
    "   - Carefully choose K through cross-validation.\n",
    "   - Use dimensionality reduction or feature selection.\n",
    "   - Implement distance-weighted voting.\n",
    "   - Use efficient data structures (e.g., KD-trees) for large datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
